// Advanced Speech-to-Text Service with multiple providers
import { pcmConverter } from './pcm-audio-converter';

export class AdvancedSpeechToText {
    private mediaRecorder: MediaRecorder | null = null;
    private audioChunks: Blob[] = [];
    private isRecording = false;
    private stream: MediaStream | null = null;

    constructor() {
        console.log('ğŸ¤ Advanced Speech-to-Text Service initialized');
    }

    // Check if browser supports audio recording
    isSupported(): boolean {
        return !!(navigator.mediaDevices &&
            typeof navigator.mediaDevices.getUserMedia === 'function' &&
            typeof window.MediaRecorder !== 'undefined');
    }

    // Start recording audio
    async startRecording(): Promise<void> {
        if (this.isRecording) {
            throw new Error('Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø¶Ø¨Ø· Ø¯Ø± Ø¬Ø±ÛŒØ§Ù† Ø§Ø³Øª');
        }

        if (!this.isSupported()) {
            throw new Error('Ù…Ø±ÙˆØ±Ú¯Ø± Ø´Ù…Ø§ Ø§Ø² Ø¶Ø¨Ø· ØµØ¯Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯');
        }

        try {
            // Get microphone access
            this.stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                    sampleRate: 16000
                }
            });

            // Clear previous chunks
            this.audioChunks = [];

            // Create MediaRecorder
            const options = {
                mimeType: 'audio/webm;codecs=opus'
            };

            // Fallback mime types if webm is not supported
            if (!MediaRecorder.isTypeSupported(options.mimeType)) {
                if (MediaRecorder.isTypeSupported('audio/mp4')) {
                    options.mimeType = 'audio/mp4';
                } else if (MediaRecorder.isTypeSupported('audio/wav')) {
                    options.mimeType = 'audio/wav';
                } else {
                    // Use default
                    delete (options as any).mimeType;
                }
            }

            this.mediaRecorder = new MediaRecorder(this.stream, options);

            // Handle data available
            this.mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    this.audioChunks.push(event.data);
                }
            };

            // Start recording
            this.mediaRecorder.start(1000); // Collect data every second
            this.isRecording = true;

            console.log('ğŸ¤ Ø´Ø±ÙˆØ¹ Ø¶Ø¨Ø· ØµØ¯Ø§...');

        } catch (error) {
            console.error('Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ Ø¶Ø¨Ø·:', error);
            throw new Error('Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ†');
        }
    }

    // Stop recording and return audio blob
    async stopRecording(): Promise<Blob> {
        if (!this.isRecording || !this.mediaRecorder) {
            throw new Error('Ø¶Ø¨Ø· Ø¯Ø± Ø¬Ø±ÛŒØ§Ù† Ù†ÛŒØ³Øª');
        }

        return new Promise((resolve, reject) => {
            if (!this.mediaRecorder) {
                reject(new Error('MediaRecorder not available'));
                return;
            }

            this.mediaRecorder.onstop = () => {
                const audioBlob = new Blob(this.audioChunks, {
                    type: this.mediaRecorder?.mimeType || 'audio/webm'
                });

                // Clean up
                this.cleanup();

                console.log('ğŸ¤ Ø¶Ø¨Ø· Ù…ØªÙˆÙ‚Ù Ø´Ø¯ØŒ Ø­Ø¬Ù… ÙØ§ÛŒÙ„:', audioBlob.size, 'bytes');
                resolve(audioBlob);
            };

            this.mediaRecorder.onerror = (event) => {
                console.error('Ø®Ø·Ø§ Ø¯Ø± Ø¶Ø¨Ø·:', event);
                this.cleanup();
                reject(new Error('Ø®Ø·Ø§ Ø¯Ø± Ø¶Ø¨Ø· ØµØ¯Ø§'));
            };

            this.mediaRecorder.stop();
            this.isRecording = false;
        });
    }

    // Convert audio to text using multiple providers
    async convertToText(audioBlob: Blob): Promise<string> {
        console.log('ğŸ”„ Ø´Ø±ÙˆØ¹ ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§ Ø¨Ù‡ Ù…ØªÙ†...');

        // Try different providers in order of preference
        const providers = [
            () => this.convertWithSahab(audioBlob), // Sahab is our primary provider
            () => this.convertWithOpenAI(audioBlob),
            () => this.convertWithGoogleSpeech(audioBlob),
            () => this.convertWithLocalAPI(audioBlob)
        ];

        for (let i = 0; i < providers.length; i++) {
            try {
                console.log(`ğŸ”„ ØªÙ„Ø§Ø´ ${i + 1}: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² provider ${providers[i].name}...`);
                const result = await providers[i]();
                if (result && result.trim()) {
                    console.log('âœ… ØªØ¨Ø¯ÛŒÙ„ Ù…ÙˆÙÙ‚:', result.substring(0, 50) + '...');
                    return result;
                }
            } catch (error) {
                console.warn(`âŒ Provider ${i + 1} failed:`, error);
                continue;
            }
        }

        throw new Error('Ù‡ÛŒÚ† Ø³Ø±ÙˆÛŒØ³ÛŒ Ù†ØªÙˆØ§Ù†Ø³Øª ØµØ¯Ø§ Ø±Ø§ Ø¨Ù‡ Ù…ØªÙ† ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ø¯');
    }

    // Convert using OpenAI Whisper API
    private async convertWithOpenAI(audioBlob: Blob): Promise<string> {
        const formData = new FormData();
        formData.append('file', audioBlob, 'audio.webm');
        formData.append('model', 'whisper-1');
        formData.append('language', 'fa'); // Persian

        const response = await fetch('/api/voice-analysis/openai-whisper', {
            method: 'POST',
            body: formData,
            credentials: 'include'
        });

        if (!response.ok) {
            throw new Error(`OpenAI API error: ${response.status}`);
        }

        const data = await response.json();
        return data.text || '';
    }

    // Convert using Web Speech API (fallback)
    private async convertWithWebSpeechAPI(audioBlob: Blob): Promise<string> {
        // This is a fallback - Web Speech API doesn't directly accept audio blobs
        // We'll use the existing enhanced Persian speech recognition
        throw new Error('Web Speech API requires live audio stream');
    }

    // Convert using Google Speech API
    private async convertWithGoogleSpeech(audioBlob: Blob): Promise<string> {
        const formData = new FormData();
        formData.append('audio', audioBlob);
        formData.append('language', 'fa-IR');

        const response = await fetch('/api/voice-analysis/google-speech', {
            method: 'POST',
            body: formData,
            credentials: 'include'
        });

        if (!response.ok) {
            throw new Error(`Google Speech API error: ${response.status}`);
        }

        const data = await response.json();
        return data.transcript || '';
    }

    // Convert using Sahab Speech Recognition API with PCM conversion
    private async convertWithSahab(audioBlob: Blob): Promise<string> {
        console.log('ğŸ¤ ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ø§ Sahab API (Ø¨Ø§ PCM)...');

        try {
            // ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ PCM Ø§Ú¯Ø± Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´ÙˆØ¯
            let base64Audio: string;

            if (pcmConverter && typeof pcmConverter.convertRecordingToPCM === 'function') {
                console.log('ğŸ”„ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ PCM...');
                base64Audio = await pcmConverter.convertRecordingToPCM(audioBlob);
                console.log('âœ… ØªØ¨Ø¯ÛŒÙ„ PCM Ù…ÙˆÙÙ‚ØŒ Ø·ÙˆÙ„ base64:', base64Audio.length);
            } else {
                console.log('âš ï¸ PCM converter Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³ØªØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Ù…Ø¹Ù…ÙˆÙ„ÛŒ...');
                base64Audio = await this.blobToBase64(audioBlob);
            }

            const response = await fetch('/api/voice-analysis/sahab-speech-recognition', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    data: base64Audio,
                    language: 'fa',
                    format: 'pcm', // Ø§Ø¹Ù„Ø§Ù… ÙØ±Ù…Øª PCM
                    sampleRate: 16000,
                    channels: 1,
                    bitDepth: 16
                }),
                credentials: 'include'
            });

            console.log('ğŸ“ Ù¾Ø§Ø³Ø® Sahab API:', response.status, response.ok);

            if (!response.ok) {
                const errorText = await response.text();
                console.error('âŒ Ø®Ø·Ø§ÛŒ Sahab API:', response.status, errorText);
                throw new Error(`Sahab API error: ${response.status} - ${errorText}`);
            }

            const data = await response.json();
            console.log('ğŸ“¥ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØªÛŒ Ø§Ø² Sahab:', data);

            if (data.success && data.data && data.data.text) {
                console.log('âœ… Ù…ØªÙ† ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡:', data.data.text);
                return data.data.text;
            }

            throw new Error('Sahab API did not return recognized text');

        } catch (error) {
            console.error('âŒ Ø®Ø·Ø§ Ø¯Ø± convertWithSahab:', error);
            throw error;
        }
    }

    // Convert base64 to blob
    private async blobToBase64(blob: Blob): Promise<string> {
        return new Promise((resolve, reject) => {
            const reader = new FileReader();
            reader.onloadend = () => {
                if (typeof reader.result === 'string') {
                    // Remove data URL prefix (e.g., "data:audio/webm;base64,")
                    const base64Data = reader.result.split(',')[1];
                    resolve(base64Data);
                } else {
                    reject(new Error('Failed to convert blob to base64'));
                }
            };
            reader.onerror = reject;
            reader.readAsDataURL(blob);
        });
    }

    // Convert using local API
    private async convertWithLocalAPI(audioBlob: Blob): Promise<string> {
        const formData = new FormData();
        formData.append('audio', audioBlob);

        const response = await fetch('/api/voice-analysis/local-speech', {
            method: 'POST',
            body: formData,
            credentials: 'include'
        });

        if (!response.ok) {
            throw new Error(`Local API error: ${response.status}`);
        }

        const data = await response.json();
        return data.text || '';
    }

    // Complete speech-to-text process
    async recordAndConvert(maxDuration: number = 30000): Promise<string> {
        console.log('ğŸ¤ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ú©Ø§Ù…Ù„ Ø¶Ø¨Ø· Ùˆ ØªØ¨Ø¯ÛŒÙ„...');

        // Start recording
        await this.startRecording();

        // Set timeout for maximum recording duration
        const timeoutPromise = new Promise<never>((_, reject) => {
            setTimeout(() => {
                reject(new Error('Ø²Ù…Ø§Ù† Ø¶Ø¨Ø· Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯'));
            }, maxDuration);
        });

        try {
            // Wait for user to stop or timeout
            await new Promise<void>((resolve) => {
                // In a real implementation, you'd have a UI button to stop recording
                // For now, we'll auto-stop after a short duration for testing
                setTimeout(() => {
                    resolve();
                }, 5000); // Auto-stop after 5 seconds for demo
            });

            // Stop recording and get audio
            const audioBlob = await this.stopRecording();

            // Convert to text
            const text = await this.convertToText(audioBlob);

            return text;

        } catch (error) {
            // Make sure to stop recording on error
            if (this.isRecording) {
                try {
                    await this.stopRecording();
                } catch (stopError) {
                    console.error('Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ‚Ù Ø¶Ø¨Ø·:', stopError);
                }
            }
            throw error;
        }
    }

    // Clean up resources
    private cleanup(): void {
        if (this.stream) {
            this.stream.getTracks().forEach(track => track.stop());
            this.stream = null;
        }
        this.mediaRecorder = null;
        this.audioChunks = [];
        this.isRecording = false;
    }

    // Get recording status
    getStatus(): {
        isRecording: boolean;
        isSupported: boolean;
        duration: number;
    } {
        return {
            isRecording: this.isRecording,
            isSupported: this.isSupported(),
            duration: this.audioChunks.length * 1000 // Approximate duration
        };
    }

    // Stop current recording (public method)
    async stop(): Promise<void> {
        if (this.isRecording) {
            await this.stopRecording();
        }
        this.cleanup();
    }
}

// Export singleton
export const advancedSpeechToText = new AdvancedSpeechToText();